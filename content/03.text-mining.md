## Building Biomedical Knowledge Graphs

1. Set up the context for relationship extraction
	1. Define the problem
	2. Talk about the importance of the problem (filling knowledge bases -> point researchers to relevant papers)
2. Give overview towards taxonomy of approaches (hand written rules, unsupervised machine learning, supervised machine learning etc.)

### Constructing Databases and Manual Curation

1. Talk about papers that construct knowledge graphs without text mining approaches
2. Discuss the positives and negatives for these methods

### Text Mining for Relationship Extraction

#### Rule-Based Relationship Extraction

Text mining via rule-based extraction consists of identifying keywords and/or grammatical patterns to detect relationship asserting sentences.
This category of approach requires a considerable amount of expert knowledge.
Usually, experts will identify grammatical pattern shared between sentences or create regular expressions to identify keywords.
There have been hybrid approaches that combine machine learning methods with these constructed patterns; however, these approaches will be discussed later sections ([unsupervised](#unsupervised-machine-learning) and [supervised](#supervised-machine-learning) machine learning).
This section discusses the pros and cons of using only rule based extraction to detect sentences of interest.

There have been studies that use pure semantic parsing to detect sentences [@doi:10.1093/nar/gkx462; @doi:10.1093/database/baw156; @doi:10.1186/s12859-018-2103-8].

<font color=red> 
Explain the study. What are the positives for identifying these pattens </font>
<font color=red> what are the cons? <- Bias results from niched findings </font>

Researchers in [@doi:10.1371/journal.pone.0152725] used a combination of pattern matching to identify mutation and gene associations.
[@doi:10.1093/bioinformatics/btg449].

<font color=red> Explain the study or group this with above. What are the positives for identifying these pattens </font>
<font color=red> what are the cons? <- Bias results from niched findings </font>

Dependency trees are resource tools tha allow researchers to understand the grammatical structure of a sentence.
This tree depicts how one word is dependent on another (Figure {@fig:dependency-parse-tree-example}).
Significant amount of approaches used dependency parse trees to extract patterns [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1093/database/bay108; @doi:10.1371/journal.pone.0152725; @doi:10.1186/1471-2105-14-181]. 

<font color=red> What are the pros of using dependency trees? </font>
<font color=red> limitation to using dependency trees?
</font>
<font color=red> Is there any open approaches that dependency trees haven't been used for yet? </font>

![
A visualization of a dependency parse tree using the following sentence as in example: "BRCA1 is associated with breast cancer".
The arrows depcit the depencency shared between two words.
For example, the dependency between BRCA1 and associated is nsubjpass.
Nsubjpass stands for passive nominal subject, which means that BRCA1 is the subject that is being referred to by the word associated.
](images/dependency_parse_example.png){#fig: dependency-parse-tree-example}

#### Unsupervised Machine Learning

1. Mention Clustering Analysis
2. Mention Co-Occurrence approaches

#### Supervised Machine Learning

1. Mention the availablility of publically available data
	1. PPI - 5 datasets 
	   1. 10.1016/j.artmed.2004.07.016 
	   2. 10.1186/1471-2105-8-50 
	   3. Learning language in logic - genic interaction extraction challenge
	   4. 10.1093/bioinformatics/btl616 
	   5. http://helix-web.stanford.edu/psb02/ding.pdf
	2. DaG - 3 datasets
	   1. 10.1016/j.jbi.2012.04.004 
	   2. 10.1186/s12859-015-0472-9
	   3. 10.1186/1471-2105-14-323 
	   4. 10.1186/1471-2105-13-161
	3. CiD 
	  1. 10.1093/database/baw068 
	4. CbG 
	  1. Biocreative VI track 5 - raw citation
	5. more if exists talk about deep learning methods
2. Mention the use of Support Vector Machines and other non deep learning classifiers
   1. Will have to mention that field has moved to deep learning.
   2. 10.1186/s13326-017-0168-3
   3. 10.1371/journal.pcbi.1004630
3. Mention deep learning methods
   1. 1901.06103v1
   2. 10.1016/j.knosys.2018.11.020
   3. 10.1177/0165551516673485
   4. 1706.01556v2
   5. ^^ A few papers here but a lot more will be put into place 
   6. Mention caveat which is the need for large annotated datasets
   7. Mention a direction the field is moving to which is weak supervision and more that info that will come in time.
