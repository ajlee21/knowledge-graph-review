## Building Biomedical Knowledge Graphs

1. Set up the context for relationship extraction
	1. Define the problem
	2. Talk about the importance of the problem (filling knowledge bases -> point researchers to relevant papers)
2. Give overview towards taxonomy of approaches (hand written rules, unsupervised machine learning, supervised machine learning etc.)

### Constructing Databases and Manual Curation

1. Talk about papers that construct knowledge graphs without text mining approaches
2. Discuss the positives and negatives for these methods

### Text Mining for Relationship Extraction

#### Rule-Based Natural Language Processing

1. Mention papers on hand written rules and expressions

#### Unsupervised Machine Learning

1. Mention Clustering Analysis
2. Mention Co-Occurrence approaches

#### Supervised Machine Learning

Supervised machine learning uses labeled data to make example input to output.
Most text mining approaches in this category range from use of support vector machines to neural network architecture.
These approaches involved a form of manual curation whether it be in house or through the use of publically available datasets (Table {@tbl:supervised-text-datasets}).
These datasets were constructed by curators for shared open tasks [@raw:biocreative/chemprot; @doi:10.1093/database/baw068] or as a means to provide a gold standard.
This section will discuss the pros and cons of popular supervised learning methods.

| Dataset | Type of Sentences |
| --- | --- |
| [@doi:10.1016/j.artmed.2004.07.016] | PPI |
| [@doi:10.1186/1471-2105-8-50] | PPI | 
| [@raw:LLL] | PPI |
| [@raw:IEPA] | PPI |
| [@doi:10.1093/bioinformatics/btl616] | PPI |
| [@doi:10.1016/j.jbi.2012.04.004] | DaG |
| [@doi:10.1186/s12859-015-0472-9] | DaG |
| [@doi:10.1186/1471-2105-14-323] | DaG | 
| [@doi:10.1186/1471-2105-13-161] | DaG |
| [@doi:10.1093/database/baw068] | Compound induces Disease |
| [@raw:biocreative/chemprot] | CbG |

Table: A set of publicaly available datasets for supervised text mining. {#tbl:supervised-text-datasets}

Common machine learning algorithms used for relationship extraction are support vector machines [@doi:10.1186/s13326-017-0168-3; @doi:10.1371/journal.pcbi.1004630] and random forest [@doi:10.1007/978-3-319-77113-7_35; @doi:10.1142/9789814366496_0040].
Support vector machines utilize kernels to project data into a high dimensional space.
This higher dimensionals space can provide a way to seperate classes and improve prediction power.
One common approach used a form of a dependency-tree kernel
to extract relationships [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503].
Other approaches have used a part of speech tree kernel  [@doi:10.1186/s13326-017-0168-3] or their own features with a regular kenel [@doi:10.1371/journal.pcbi.1004630] to perform extraction.
A recent approach used an ensemble of svms to extract disease-gene associations [@doi:10.1371/journal.pone.0200699].
These methods provide powerful precision in terms of extraction, but heavily rely on feature construction for performance.
Also, support vector machines take a significant amount of time to run as the size of the dataset linearly increases.
This makes it infeasible to use if approachers were to undergo extraction using full text.

Deep learning is an increasingly popular field that is used by a lot of text mining approaches. 
Algorithms in this category range from recurrent neural networks to convolutional neural networks.
Recurrent neural networks allow sequential analysis where a single hidden state is updated as the network moves along a sequence. 
A notable recurrent neural network is a long short term memory network that is able to capture long distant relationships [@doi:10.1162/neco.1997.9.8.1735].
Other work that uses a recurrent neural network can be found in table {@tbl:deep-learning-text-mining}.

Convolutional neural networks are famous for image analysis.
These networks use a kernel filter to extract subset of features within a data matrix.
These filters create a feature map that a feedforward network will utilize to make classification predictions [@doi:10.1038/nature14539].
Recent work for using this network can be found in table {@tbl:deep-learning-text-mining}.
Despite the amount of success these networks have, they need a non-trivial amount of data to peform well.
Many of the publically available datasets won't be able to provide sufficient power given this necessity.

Based on the amount of data bottleneck, work have taken to semi-supervised or weakly supervised methods. 
Semi-supervised learning combines labeled data with unlabeled data to accomplish a task.
One work used a variational auto encoder with a LSTM network to extract protein-protein interactions from pubmed abstracts and full text [@doi:1901.06103v1].
Distant supervision is an approach that uses supervised machine learning to perform extraction; however, labels for training data could be incorrect [@doi:10.3115/1690219.1690287].
The assumption here is that these algorithms can learn through noise and still have comparable performance.
Work related towards this paradigm can be found here: 
[@doi:10.1093/bioinformatics/btv476; @doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898; @doi:10.1101/730085].
As publications continue to rise the approach appears to be promising.

<font color=red> Insert conclusion to wrap tup this section. </font>

| Reference | Model | Extraction Goal | 
| --- | --- | --- |
| [@arxiv:1408.5882] | CNN | |
| [@doi:10.1177/0165551516673485] | CNN | |
| [@arxiv:1706.01556v2] | CNN | |
| [@doi:10.1016/j.knosys.2018.11.020] | LSTM | |
| [@arxiv:1708.03743] | LSTM | |
| [@arxiv:1805.07683] | LSTM | |
| [@arxiv:1808.09101] | LSTM | |
| [@doi:10.1016/j.knosys.2018.11.020] | LSTM | |

Table: A set of works that use deep learning methods to perform realtion extraction. {#tbl:deep-learning-text-mining}
