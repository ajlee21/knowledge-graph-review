## Building Biomedical Knowledge Graphs

1. Set up the context for relationship extraction
	1. Define the problem
	2. Talk about the importance of the problem (filling knowledge bases -> point researchers to relevant papers)
2. Give overview towards taxonomy of approaches (hand written rules, unsupervised machine learning, supervised machine learning etc.)

### Constructing Databases and Manual Curation

1. Talk about papers that construct knowledge graphs without text mining approaches
2. Discuss the positives and negatives for these methods

### Text Mining for Relationship Extraction

#### Rule-Based Natural Language Processing

1. Mention papers on hand written rules and expressions

#### Unsupervised Machine Learning

1. Mention Clustering Analysis
2. Mention Co-Occurrence approaches

#### Supervised Relationship Extraction

Supervised extraction uses labeled relationships to learn text patterns that correspond to positively labeled relationships instead of negative ones.
Most of these approaches have flourished due to pre-labelled publicly available datasets (Table {@tbl:supervised-text-datasets}).
These datasets were constructed by curators for shared open tasks [@raw:biocreative/chemprot; @doi:10.1093/database/baw068] or as a means to provide the scientific community with a gold standard [@doi:10.1093/bioinformatics/btl616; @doi:10.1093/database/baw068; @doi:10.1186/1471-2105-14-323].
Approaches that use these available datasets range from using support vector machines (SVMs) with custom kernels to deep learning with algorithms that can construct their own features.
In the rest of this section we discuss approaches that use supervised methods to detect relationship-asserting sentences.

Extracting relationships in a supervised setting can involve mapping textual input onto a high dimensional space.
Support vector machines are a type of classifier that can accomplish this task with a mapping function called a kernel [@doi:10.1186/s13326-017-0168-3; @doi:10.1371/journal.pcbi.1004630].
These kernels take information such as a sentence's dependency tree [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503], part of speech tags [@doi:10.1186/s13326-017-0168-3] or even word counts [@doi:10.1371/journal.pcbi.1004630] and map them onto a dense feature space.
Within this space, the methods learn a hyperplane that separates sentences in the positive class (mentions a relationship) from the negative class (does not mention a relationship). 
Kernels can be manually constructed or selected to cater to the relationship being extracted [@doi:10.1186/s13326-017-0168-3; @doi:10.1093/bioinformatics/btw503;@doi:10.1371/journal.pcbi.1004630; @doi:10.1371/journal.pcbi.1004630].
Determining the correct kernel requires expert knowledge to be successful and is a nontrivial task depending on the relationship.
In addition to single kernel methods, a recent study used an ensemble of SVMs to extract disease-gene associations [@doi:10.1371/journal.pone.0200699].
The ensemble outperformed notable disease-gene association extractors [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1186/s12859-015-0472-9] in terms of precision, recall and F1 score.
Overall, future SVM methods should consider a more automated approaches to define these kernel functions or develop ways to use semi-automatic methods for kernel construction.

Deep learning is an increasingly popular class of techniques that can construct their own features within a high dimensional space [@raw:GoodfellowDL; @doi:10.1038/nature14539].
These methods use different forms of neural networks, such as recurrent or convolutional neural networks, to perform classification.

Recurrent neural networks (RNN) are designed for sequential analysis that consist of using a repeatedly updating hidden state to make predictions.
An example of a recurrent neural network is a long short term memory (LSTM) network [@doi:10.1162/neco.1997.9.8.1735].
Cocos et al [@doi:10.1093/jamia/ocw180] used a LSTM to extract drug side effects from de-identified twitter posts, while Yadav et al. [@doi:10.3233/978-1-61499-830-3-644] used an LSTM to extract protein-protein interactions.
Other works have used LSTMs to perform relationship extraction [@arxiv:1708.03743; @doi:10.1093/bioinformatics/btw486; @arxiv:1808.09101; @doi:10.1186/s12859-017-1609-9; @doi:10.1093/jamia/ocw180]. 
Despite the success of these networks, training can be difficult as these networks are highly susceptible to vanishing and exploding gradients [@doi:10.1109/ICNN.1993.298725; @arxiv:1808.03314].
One solution to this problem is to clip the gradients while the neural network trains [@arxiv:1211.5063].
Besides the gradient problem, these approaches peak in performance when the dataset reaches at least a tens of thousand of data points [@arxiv:1707.02968].

Convolutional neural networks (CNN) are heavily used in image analysis and consist of using multiple kernel filters to capture small subsets of the overall image [@doi:10.1038/nature14539].
In the context of text mining an image is replaced with words within a sentence mapped to dense vectors (i.e., word embeddings) [@arxiv:1301.3781; @arxiv:1310.4546].
Peng et al. [@arxiv:1706.01556v2] used a CNN to extract sentences that mentioned protein-protein interactions and Zhou et al. [@doi:10.1186/s12859-019-2873-7] used a CNN to extract chemical-disease relations.
Other approaches have used CNNs and variants of CNNs to extract relationship-asserting sentences [@doi:10.1177/0165551516673485; @doi:10.1093/database/bay073; @doi:10.1101/730085].
Just like RNNs, these networks perform well when millions of labeled examples are present [@arxiv:1707.02968]. 
Future approaches that use CNNs or RNNs should consider solutions to obtaining these large quantities of data through means such as weak supervision [@doi:10.3115/1690219.1690287], semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] or using pre-trained networks via transfer learning [@doi:10.1109/TKDE.2009.191; @doi:10.1186/s40537-016-0043-6].

Semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] and weak supervision [@doi:10.3115/1690219.1690287] are techniques that can construct large datasets for machine learning classifiers. 
Semi-supervised learning consists of combining labeled data with unlabeled data to extract relationships.
For example, one study used a variational auto encoder with a LSTM network to extract protein-protein interactions from pubmed abstracts and full text [@arxiv:1901.06103v1].
This is an elegant solution to handle the small dataset problem, but requires labeled data to start. 
The dependency on labeled data makes finding under-studied relationships difficult as one would need to find or construct examples of the missing relationships in the beginning.

Weak or distant supervision takes a different approach that uses noisy or even erroneous labels to train classifiers [@doi:10.3115/1690219.1690287; @doi:10.1093/bioinformatics/btv476; @doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898].
Under this paradigm sentences are labeled based on their mention pair being present (positive) or absent (negative) in a database.
Once these labels are obtained a machine learning classifier can now be trained to predict sentences [@doi:10.3115/1690219.1690287].
For example, Thomas et al. [@raw:Thomas2011LearningTE] used distant supervision to train a support vector machine to extract sentences mentioning protein-protein interactions (ppi). 
Their SVM model achieved comparable performance against a baseline model; however, the noise generated via distant supervision was difficult to eradicate [@raw:Thomas2011LearningTE].
Some approaches have attempted to fix this noise problem [@arxiv:1805.09927; @arxiv:1805.09929; @doi:10.18653/v1/W17-2323], but it still remains an open problem to be solved.
Overall, semi-supervised learning and weak supervision provide promising results in terms of relation extraction and future approaches should consider using those paradigms to train machine learning classifiers.

| Dataset | Type of Sentences |
| --- | --- |
| [@doi:10.1016/j.artmed.2004.07.016] | PPI |
| [@doi:10.1186/1471-2105-8-50] | PPI | 
| [@raw:LLL] | PPI |
| [@raw:IEPA] | PPI |
| [@doi:10.1093/bioinformatics/btl616] | PPI |
| [@doi:10.1016/j.jbi.2012.04.004] | DaG |
| [@doi:10.1186/s12859-015-0472-9] | DaG |
| [@doi:10.1186/1471-2105-14-323] | DaG | 
| [@doi:10.1186/1471-2105-13-161] | DaG |
| [@doi:10.1093/database/baw068] | Compound induces Disease |
| [@raw:biocreative/chemprot] | CbG |

Table: A set of publicly available datasets for supervised text mining. {#tbl:supervised-text-datasets}
