## Building Biomedical Knowledge Graphs

1. Set up the context for relationship extraction
	1. Define the problem
	2. Talk about the importance of the problem (filling knowledge bases -> point researchers to relevant papers)
2. Give overview towards taxonomy of approaches (hand written rules, unsupervised machine learning, supervised machine learning etc.)

### Constructing Databases and Manual Curation

1. Talk about papers that construct knowledge graphs without text mining approaches
2. Discuss the positives and negatives for these methods

### Text Mining for Relationship Extraction

#### Rule-Based Natural Language Processing

1. Mention papers on hand written rules and expressions

#### Unsupervised Machine Learning

1. Mention Clustering Analysis
2. Mention Co-Occurrence approaches

#### Supervised Machine Learning

Supervised machine learning uses labeled data to learn patterns and map example input to desired output.
Most text mining approaches under this category have flourished due to publically available datasets (Table {@tbl:supervised-text-datasets}).
Most of these datasets were constructed by curators for shared open tasks [@raw:biocreative/chemprot; @doi:10.1093/database/baw068] or as a means to provide the scientific community with a gold standard.
In this section we discuss the pros and cons of using supervised machine learning methods to detect sentences asserting a relationship.

| Dataset | Type of Sentences |
| --- | --- |
| [@doi:10.1016/j.artmed.2004.07.016] | PPI |
| [@doi:10.1186/1471-2105-8-50] | PPI | 
| [@raw:LLL] | PPI |
| [@raw:IEPA] | PPI |
| [@doi:10.1093/bioinformatics/btl616] | PPI |
| [@doi:10.1016/j.jbi.2012.04.004] | DaG |
| [@doi:10.1186/s12859-015-0472-9] | DaG |
| [@doi:10.1186/1471-2105-14-323] | DaG | 
| [@doi:10.1186/1471-2105-13-161] | DaG |
| [@doi:10.1093/database/baw068] | Compound induces Disease |
| [@raw:biocreative/chemprot] | CbG |

Table: A set of publicaly available datasets for supervised text mining. {#tbl:supervised-text-datasets}

Most text mining approaches used support vector machines (SVM) [@doi:10.1186/s13326-017-0168-3; @doi:10.1371/journal.pcbi.1004630] to perform relationship extraction.
SVMs use a hyperplane or set of hyperplanes to bisect datapoints for classification.
Typically SVMs utilize kernels to construct these hyperplanes.
A kernel is a function that maps raw data into high dimensional user specified feature vectors.
Common kernels are: dependency tree aware kernels [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503], part of speech kernels to generate features [@doi:10.1186/s13326-017-0168-3] and kernels that rely on user generated features such as word counts [@doi:10.1371/journal.pcbi.1004630].
Kernel approaches heavily rely on expert knowledge to be successful, which results in highly precise results and low recall of related sentences.
Besides kernel methods, a recent study used an ensemble of SVMs to extract disease-gene associations [@doi:10.1371/journal.pone.0200699].
This methods provide powerful precision in terms of extraction and outperformed the previous baseline model.
Overall, approaches that use support vector machines provide great results, but require expert knowledge to get comparable performance.
Plus, SVMs can take a significant amount of time to run if a dataset gets to monstrous size.
This bottleneck creates a scalability issue for using solely SVMs to perform relationship extraction.

Deep learning is an increasingly popular field used in a lot of recent text mining approaches.
These approaches use different forms of neural networks, such a recurrent neural network or a convolutional neural network, to perform classifcation.
These networks are able to automatically construct features and provide powerful detection performance, but are very esoteric in terms of fully understanding how they work.

Recurrent neural networks (RNN) are designed for sequential analysis.
These network have a single latent state that is updated as new input is introduced. 
An example of a recurrent neural network is a long short term memory (LSTM) network.
This network is designed to capture that can capture long distant relationships from sequential data [@doi:10.1162/neco.1997.9.8.1735].
Sentences can be considered a sequence of words, which comlements this network well.
For example one study used a LSTM to extract drug side effects from de-identified twitter posts [@doi:10.1093/jamia/ocw180].
More studies that use recurrent neural networks can be found in table {@tbl:deep-learning-text-mining}.
Despite the success of these networks, runtime can be an issue.
For example, using a LSTM on a dataset that has 200+ token sentences can take up to days to run.
Based on this caution should be taken for future approaches that use this network.

Convolutional neural networks (CNN) are heavily used in image analysis.
These networks use kernel filters to generate multiple features in a dataset.
These generated features are then consolidated by a feedforward network for classification [@doi:10.1038/nature14539].
One example used a CNN to extract sentences concerning  protein-protien intercations [@arxiv:1706.01556v2].
Other recent work using this network can be found in table {@tbl:deep-learning-text-mining}.
Just like RNNs, these networks need a large amount of data to peform well.
This is a non-trivial problem because creating these datasets is time consuming and hihgly prone to errors.

Since neural networks need a lot of data to perform well, the field has turned to using semi-supervised or weakly supervised machine learning. 
Semi-supervised learning combines labeled data with unlabeled data to accomplish a task.
For example, one study used a variational auto encoder with a LSTM network to extract protein-protein interactions from pubmed abstracts and full text [@doi:1901.06103v1].
This is an elogant solution to handle the small dataset problem, but requires to have labeled data to start. 
The dependency on labeled data makes finding under-studied relationships diffcult as one would need to hand craft a dataset at the start.

Weak or distant supervision is an approach that uses noisy or even erroneous labels to perform supervised relation extraction [@doi:10.3115/1690219.1690287].
Algorithms under this paradigm can learn through this noise and produce comparable performance.
Notable works related to this paradigm can be found here: 
[@doi:10.1093/bioinformatics/btv476; @doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898; @doi:10.1101/730085].
These approaches provide promising results in terms of relation extraction.
Some methods perform nearly as well as some baseline approaches, which suggests that future methods should consider using this paradigm to perform extraction.

| Reference | Model | Relationship Extraction Type | 
| --- | --- | --- |
| [@doi:10.1093/database/bay073] | CNN | Chemical-Protein Interactions|
| [@doi:10.1177/0165551516673485] | CNN | Protein-Protein Interactions |
| [@arxiv:1706.01556v2] | CNN | Protein-Protein Interactions |
| [@doi:10.1186/s12859-019-2873-7] | CNN | Chemical-Disease Relations |
| [@doi:10.1101/730085] | CNN | Drug-Disease treatments, Drug-Gene bindings, Disease-Gene associations and Protein-Protein interactions|
| [@doi:10.1016/j.knosys.2018.11.020] | LSTM | Protein-Protein interactions |
| [@arxiv:1708.03743] | LSTM | Drug-Gene interactions|
| [@doi:10.1093/bioinformatics/btw486] | LSTM | Drug-Drug interactions|
| [@arxiv:1808.09101] | LSTM | Drug-Gene interactions |
| [@doi:10.1186/s12859-017-1609-9] | LSTM | Drug Side Effects | 
| [@doi:10.1093/jamia/ocw180] | LSTM | Drug Side Effects |

Table: A listing of text mining approaches that use deep learning to perform relation extraction. {#tbl:deep-learning-text-mining}
