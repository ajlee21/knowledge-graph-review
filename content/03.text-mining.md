## Building Biomedical Knowledge Graphs

Knowledge graphs can be constructed in many ways using resources such as text or pre-existing databases. 
Usually, knowledge graphs are constructed using pre-existing databases.
These databases are constructed by domain experts using approaches ranging from manual curation to automated techniques, such as text mining systems.
Manual curation is a process that involves extensive use of domain experts to read papers and detect sentences that assert a relationship.
Automated approaches involve the use of machine learning or natural language processing techniques to rapidly detect sentences of interest.
We categorize these automated approaches into the following groups: rule-based extraction, unsupervised machine learning, and supervised machine learning.
We discuss examples of each type of approach and synthesize the strengths and weaknesses of each.

### Constructing Databases and Manual Curation

Database construction can date back all the way to 1956 where the first database contained a protein sequence of the insulin molecule [@doi:10.4172/jcsb.1000081].
This process involves gathering relevant text such as journal articles, abstracts, or web-based text.
At this point curators can read gathered text and detect relationship asserting sentences (i.e. relationship extraction).
An alternative to use a text mining system to filter out extraneous sentences, then incorporate curators to perfect the system's findings. 
This semi-automatic approach is way to augment curators throughout the curation process.
We discuss the pros and cons of using manual curation for relationship extraction and mention databases that use this method to populate their fields.

Notable databases have been constructed via manual curation (Table {@tbl:manual-curated-databases}).
For example, COSMIC [@doi:10.1093/nar/gkw1121] was constructed via a group of domain experts scanning the literature for key cancer related genes.
This database has reached close to 35M entries in 2016 [@doi:10.1093/nar/gkw1121] and grew to a total of 45M entiries in 2019 [@doi:10.1093/nar/gky1015].
Studies have shown that these databases contain relatively precise data, but in low quantities [@doi:10.1038/nmeth1209-860; @doi:10.1038/nmeth.1284; @doi:10.1093/database/bau058; @doi:10.1093/nar/gku1205; @doi:10.1186/s12859-018-2103-8; @doi:10.1093/database/bax043; @doi:10.1093/bioinformatics/btm229].
This happens because the high publication rate is too much for curators to keep up [@doi:10.1007/s11192-010-0202-z].
This findings highlight a critical need for future approaches to be fast enough to compete with an increasing publication rate.

Semi-automatic methods are a way to augment curators during the curation process [@doi:10.1016/j.jbi.2010.11.001; @doi:10.1093/database/bau067; @doi:10.1093/database/bas010; @doi:10.1145/3269206.3269229; @doi:10.1093/database/baz068; @doi:10.1186/s12859-018-2103-8; @doi:10.1186/s12859-018-2021-9].
First step in this context is to use an automatic system to initally extract sentences from text.
This process filters out irrelevant sentences, which means less text for  curators to sift through.
After the pre-filtering step curators can approve or remove the identified sentences.
This semi-automatic process was found to speed up the curation process compared to manual approach [@doi:10.1016/j.jbi.2010.11.001; @doi:10.1136/amiajnl-2013-001837]. 
Curators in [@doi:10.1136/amiajnl-2013-001837] saved an average of 2.8 hours of overall time while curators in [@doi:10.1016/j.jbi.2010.11.001] saved about the same amount of time (2 hours). 
Despite the speed up, this process is prone to produce bias results.
As automated systems excel in identifying sentences for commonly occurring relationships, they miss out on lessor known relationships [@doi:10.1016/j.jbi.2010.11.001].
Plus, these systems have a hard time parsing ambiguous sentences that naturally occur in text.
This complication results in curators have a difficult time correcting these systems [@doi:10.1016/j.jbi.2010.11.001].
Given these caveats, a future direction would be using or creating approaches that can mitigate the relationship bias.
Furthermore, future approaches should look into using techniques that simplify sentences to solve the ambiguity issue [@doi:10.1093/database/bau038; @pmid:21346999].

Despite the negatives of manual curation, it is still an essential process for relationship extraction approaches.
This process can be used to generate gold standard datasets that automated systems use for validation [@doi:10.1016/j.jbi.2012.04.004; @doi:10.1016/j.artmed.2004.07.016].
Furthermore, manual curation can be used during the training process of  automated systems (i.e. active learning) [@doi:10.1007/s11390-012-1306-0].
It is important to remember that manual curation alone is precise, but results in low recall rates [@doi:10.1093/bioinformatics/btm229].
Future databases should consider initially relying on automated methods to obtain sentences at an acceptable recall level, then incorporate manual curation as a way to fix or remove irrelevant results.

| Database [Reference] | Short Description | Number of Entries | Entity  Types | Relationship Types | Method of Population |
| --- | --- | --- | --- | --- | --- |
| Entrez-Gene [@doi:10.1093/nar/gkq1237] | NCBI's Gene annotation database that contains information pertaining to genes, gene's organism source, phenotypes etc. | 7,883,114 | Genes, Species and Phenotypes | Gene-Phenotypes and Genes-Species mappings | Semi-automated curation |
| UniProt [@doi:10.1093/nar/gky1049] | A protein protein interaction database that contains proteomic information. | 560,823 | Proteins, Protein sequences | Protein-Protein interactions | Manual and Automated Curation | 
| PharmGKB [@doi:10.1038/clpt.2012.96] | A database that contains genetic, phenotypic, and clinical information related to pharmacogenomic studies. | 43,112 | Drugs, Genes, Phenotypes, Variants, Pathways | Gene-Phenotypes, Pathway-Drugs, Gene-Variants, Gene-Pathways | Manual Curation and Automated Methods |
| COSMIC [@doi:10.1093/nar/gkw1121] | A database that contains high resolution human cancer genetic information. | 35,946,704 | Genes, Variants, Tumor Types| Gene-Variant Mappings | Manual Curation | 
| BioGrid [@doi:10.1093/nar/gks1158] | A database for major model organisms. It contains genetic and proteomic information.| 572,084 | Genes, Proteins| Protein-Protein interactions | Semi-automatic methods |
| Comparative Toxicogenomics Database [@doi:10.1093/nar/gky868] | A database that contains manually curated chemical-gene-disease interactions and  relationships. | 2,429,689 | Chemicals (Drugs), Genes, Diseases | Drug-Genes, Drug-Disease, Disease-Gene mappings | Manual curation and Automated systems |
| Comprehensive Antibiotic Resistance Database [@doi:10.1093/nar/gkw1004] | Manually curated database that contains information about the molecular basis of antimicrobial resistance. | 174,443 | Drugs, Genes, Variants | Drug-Gene, Drug-Variant mappings | Manual curation |
| OMIM [@pmid:30445645] | A database that contains phenotype and genotype information | 25,153 | Genes, Phenotypes | Gene-Phenotype mappings | Manual Curation |
Table. A table of databases that used a form of manual curation to populate entries. 
Reported number of entities and relationships are relative to time of publication.
{#tbl:manual-curated-databases}

### Text Mining for Relationship Extraction

#### Rule-Based Relationship Extraction

Rule-based extraction consists of identifying sentences that contain important keywords or grammatical patterns that allude to relationships of interest. 
Keywords are established via expert knowledge or though the use of pre-existing ontologies.
Grammatical patterns are constructed via experts curating parse trees, which are tree data structures that depict a sentence's grammatical structure.
Parse trees come into two forms a constituency parse tree and a dependency parse trees.
Both trees use part of speech tags, labels that dictate the grammatical role of a word such as noun, verb, adjective, etc, for construction.
A constituency parse trees breaks a sentence down into a subphrases (Figure {@fig:constituency-parse-tree-example}) while dependency path trees analyzes the grammatical structure of a sentence (Figure {@fig:dependency-parse-tree-example}).
Many text mining approaches [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503; @doi:10.1186/s13326-017-0168-3] use such trees to generate features for machine learning algorithms.
These approaches are discussed in later sections.
For this section we focus on approaches that mainly use rule based extraction to detect sentences that assert a relationship.

Grammatical patterns can simplify sentences for easy extraction [@pmid:21346999; @pmid:24850848].
Jonnalagadda et al. used a set of grammar rules inspired by constituency trees to reshape complex sentences with simpler versions [@pmid:21346999].
These simplified versions were manually curated to determine the presence of a relationship.
By simplyfing sentences this approach achieved high recall, but had low precision [@pmid:21346999].
Other approach used simplification techniques to make extraction easier [@doi:10.1093/database/baw156; @doi:10.1186/1471-2105-15-285; @doi:10.1093/database/bav020; @doi:10.1371/journal.pcbi.1004391].
Tudor et al., simplified sentences to detect protein phosphorylation events [@doi:10.1093/database/bav020].
The sentence simplifier broke complex sentences that contain multiple protein events into smaller sentences that contain only one distinct event.
By breaking these sentences down the authors were able to increase their recall.
However, sentences that contained ambigious directionality or multiple phosphroylation events were too complex for the simplifier.
As a consequence the simplifier produced errors in recall [@doi:10.1093/database/bav020].
These errors highlight a crucial need for future algorithms to be generalizable enough to handle various forms of complex sentences.

Pattern matching is a fundamental approach used to detect relationship asserting sentences.
In this context patterns can consist of phrases from constituency trees, a set of keywords or some combination of both to detect sentences [@doi:10.1093/nar/gkx462; @doi:10.1186/s12859-018-2103-8; @doi:10.1371/journal.pone.0152725; @doi:10.1093/bioinformatics/btg449; @doi:10.1186/1471-2105-14-181; @doi:10.1109/TCBB.2014.2372765].
Xu et al. designed a pattern matcher system to detect sentences in PubMed abstracts that indicate drug-disease treatments [@doi:10.1186/1471-2105-14-181].
This system matched drug-disease pairs from clinicaltrails.gov to drug-disease pairs mentioned in abstracts.
This matching process aided the authors in identifying sentences that were used to create simple patterns, such as "Drug in the treatment of Disease" [@doi:10.1186/1471-2105-14-181], to match sentences in a wide variety of abstracts.
The authors hand curated two datasets for evalution and achieved a high precision score of 0.904 and a low recall score of 0.131 [@doi:10.1186/1471-2105-14-181].
This low recall score was based on constructed patterns being very specific to top occurring drug paris.
This flaw resulted in rarely occurring pairs having a high likelihood of being missed.
Following approaches using constituency trees, some approaches used dependency trees to construct patterns [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1093/database/bay108].
Depending upon the nature of the algorithm, dependency trees could be more appropiate than constituency trees and vise versa.
The performance difference between the two approaches still remains as an open question for future exploration.

Rules based methods provide a basis for many relationship extraction systems.
Approaches in this category range from simplifing sentences for easy extraction to identifing sentences based on matched key phrases or grammatical patterns.
Both require a significant amount of manual effort and expert knowledge to perform well.
A future direction is to develop ways to automatically construct these hand-crafted patterns, which would accelerate the process of creating new rule-based systems.

![
A visualization of a dependency parse tree using the following sentence as in example: "BRCA1 is associated with breast cancer" [@raw:honnibal2017spacy].
For these type of trees the root begins at the main verb of a sentence.
Each arrows depicts the dependency shared between two words.
For example, the dependency between BRCA1 and associated is nsubjpass, which stands for passive nominal subject.
This means that BRCA1 is the subject of the sentneces and it is being referred to by the word associated.
](images/figures/dependency_parse_example.png){#fig:dependency-parse-tree-example}

![
A visualization of a constituency parse tree using the following sentence: "BRCA1 is associated with breast cancer" [@raw:eisenbach2006phpsyntaxtree].
This type of tree has the root beginning at the start of the sentence.
Each word is grouped into subphrases depending on the part of speech tags of a word.
For example, the word "associated" is a past particple verb (VBN) that belongs to the verb phrase (VP) subgroup.
](images/figures/constituency_parse_tree_example.png){#fig:constituency-parse-tree-example}

#### Extracting Relationships Without Labels

Unsupervised methods of extraction involve drawing inferences from data without the use of labels.
These methods involve some form of clustering or statistical calculations.
In this section we discuss methods that use unsupervised learning to detect relationship asserting sentences from text.

An unsupervised method to extract relationships exploits the fact that two entities can appear together in text. 
This kind of event is called co-occurrence and studies that use this phenomenon can be found in table {@tbl:unsupervised-methods-text-mining}.
Two databases DISEASES [@doi:10.1016/j.ymeth.2014.11.020] and STRING [@doi:10.1093/nar/gks1094] were populated using a co-occurrence scoring method on PubMed abstracts.
Both databases used the same scoring method that measured the frequency of co-mention pairs within individual sentences as well as the abstracts themselves.
This method assumes independence between each individual occurrence.
Under this assumption mention pairs that occur more than expected were presumed to indicate the presence of an association or interaction.
This approach was able to identify 543,405 disease gene associations [@doi:10.1016/j.ymeth.2014.11.020] and 792,730 high confidence protein protein interactions [@doi:10.1093/nar/gks1094], but is limited to only using PubMed abstracts.

Full text articles are able to drastically amplify text mining power to detect relationships [@doi:10.1371/journal.pcbi.1005962; @doi:10.1093/nar/gkt1207].
Westergaard et al. used a co-occurrence approach, similar to DISEASES [@doi:10.1016/j.ymeth.2014.11.020] and STRING [@doi:10.1093/nar/gks1094], to mine full articles for protein-protein interactions and other protein related information [@doi:10.1371/journal.pcbi.1005962].
The authors discovered that full text provided better prediction power than using abstracts alone.
This improvement suggests that future text mining approaches should consider using full text to increase detection power.

Unsupervised methods have been focused on treating multiple biomedical relationships as multiple isolated problems.
These methods repeatedly use the same model for each biomedical relationship type.
An alternative to this persepctive is to capture all different relationship types at once.
Clustering is an approach that accomplish this concept of simultaneous extraction.
Percha et al. used a biclustering algorithm on generated dependency parse trees to group PubMed abstract sentences [@doi:10.1093/bioinformatics/bty114].
Each cluster was manually curated to determine which relationship they represented.
This approach captured 4,451,661 dependency paths for 36 different groups [@doi:10.1093/bioinformatics/bty114].
Despite the success, this approach suffered from technical issues such as dependency tree parsing errors.
This type of error resulted in sentences not being grouped by the clustering algorithm [@doi:10.1093/bioinformatics/bty114].
Future clustering approaches should consider simplifying sentences to prevent this type of issue.

Overall unsupervised methods provide a means to rapidly find relationship asserting sentences without the need of annotated text.
Approaches in this category range from using co-occurrence scores to clustering sentences.
These methods provide a generalizable framework that can be used on large repositories of text.
Future methods can improve detection power by considering the use of methods that simplify sentences and use datasets that include full text articles.

| Study | Relationship of Interest | 
| --- | --- | 
| [@doi:10.1093/bioinformatics/btz490] | Protein-Protein Interactions, Disease-Gene and Tissue-Gene Associations |
| [@doi:10.1109/bibm.2015.7359766] | Drug Disease Treatments |
| [@doi:10.1371/journal.pcbi.1000943] | Drug, Gene and Disease interactions |
| [@doi:10.1371/journal.pcbi.1005962]| Protein-Protein Interactions |
| [@doi:10.1016/j.ymeth.2014.11.020] | Disease-Gene associations|
| [@doi:10.1093/nar/gku1003]| Protein-Protein Interactions |
| [@doi:10.1371/journal.pcbi.1005017] | Genotype-Phenotype Relationships |

Table: Table of approaches that mainly use a form of co-occurrence. {#tbl:unsupervised-methods-text-mining}

#### Supervised Relationship Extraction

Supervised extraction uses labeled relationships to learn text patterns that correspond to positively labeled relationships instead of negative ones.
Most of these approaches have flourished due to pre-labelled publicly available datasets (Table {@tbl:supervised-text-datasets}).
These datasets were constructed by curators for shared open tasks [@raw:biocreative/chemprot; @doi:10.1093/database/baw068] or as a means to provide the scientific community with a gold standard [@doi:10.1093/bioinformatics/btl616; @doi:10.1093/database/baw068; @doi:10.1186/1471-2105-14-323].
Approaches that use these available datasets range from using support vector machines (SVMs) with custom kernels to deep learning with algorithms that can construct their own features.
In the rest of this section we discuss approaches that use supervised methods to detect relationship-asserting sentences.

Extracting relationships in a supervised setting can involve mapping textual input onto a high dimensional space.
Support vector machines are a type of classifier that can accomplish this task with a mapping function called a kernel [@doi:10.1186/s13326-017-0168-3; @doi:10.1371/journal.pcbi.1004630].
These kernels take information such as a sentence's dependency tree [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503], part of speech tags [@doi:10.1186/s13326-017-0168-3] or even word counts [@doi:10.1371/journal.pcbi.1004630] and map them onto a dense feature space.
Within this space, the methods learn a hyperplane that separates sentences in the positive class (mentions a relationship) from the negative class (does not mention a relationship). 
Kernels can be manually constructed or selected to cater to the relationship being extracted [@doi:10.1186/s13326-017-0168-3; @doi:10.1093/bioinformatics/btw503;@doi:10.1371/journal.pcbi.1004630; @doi:10.1371/journal.pcbi.1004630].
Determining the correct kernel requires expert knowledge to be successful and is a nontrivial task depending on the relationship.
In addition to single kernel methods, a recent study used an ensemble of SVMs to extract disease-gene associations [@doi:10.1371/journal.pone.0200699].
The ensemble outperformed notable disease-gene association extractors [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1186/s12859-015-0472-9] in terms of precision, recall and F1 score.
Overall, SVMs have been shown to be beneficial in terms of relationship mining; however, major focus have shifted to utilizing deep learning techniques to extract relationships as these approaches can perform non-linear mappings of high dimensional data.

Deep learning is an increasingly popular class of techniques that can construct their own features within a high dimensional space [@raw:GoodfellowDL; @doi:10.1038/nature14539].
These methods use different forms of neural networks, such as recurrent or convolutional neural networks, to perform classification.

Recurrent neural networks (RNN) are designed for sequential analysis that consist of using a repeatedly updating hidden state to make predictions.
An example of a recurrent neural network is a long short term memory (LSTM) network [@doi:10.1162/neco.1997.9.8.1735].
Cocos et al [@doi:10.1093/jamia/ocw180] used a LSTM to extract drug side effects from de-identified twitter posts, while Yadav et al. [@doi:10.3233/978-1-61499-830-3-644] used an LSTM to extract protein-protein interactions.
Other works have used LSTMs to perform relationship extraction [@arxiv:1708.03743; @doi:10.1093/bioinformatics/btw486; @arxiv:1808.09101; @doi:10.1186/s12859-017-1609-9; @doi:10.1093/jamia/ocw180]. 
Despite the success of these networks, training can be difficult as these networks are highly susceptible to vanishing and exploding gradients [@doi:10.1109/ICNN.1993.298725; @arxiv:1808.03314].
One solution to this problem is to clip the gradients while the neural network trains [@arxiv:1211.5063].
Besides the gradient problem, these approaches peak in performance when the dataset reaches at least a tens of thousand of data points [@arxiv:1707.02968].

Convolutional neural networks (CNNs), which are widely applied for image analysis, use multiple kernel filters to capture small subsets of an overall image [@doi:10.1038/nature14539].
In the context of text mining an image is replaced with words within a sentence mapped to dense vectors (i.e., word embeddings) [@arxiv:1301.3781; @arxiv:1310.4546].
Peng et al. [@arxiv:1706.01556v2] used a CNN to extract sentences that mentioned protein-protein interactions and Zhou et al. [@doi:10.1186/s12859-019-2873-7] used a CNN to extract chemical-disease relations.
Other approaches have used CNNs and variants of CNNs to extract relationship-asserting sentences [@doi:10.1177/0165551516673485; @doi:10.1093/database/bay073; @doi:10.1101/730085].
Just like RNNs, these networks perform well when millions of labeled examples are present [@arxiv:1707.02968]. 
Future approaches that use CNNs or RNNs should consider solutions to obtaining these large quantities of data through means such as weak supervision [@doi:10.3115/1690219.1690287], semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] or using pre-trained networks via transfer learning [@doi:10.1109/TKDE.2009.191; @doi:10.1186/s40537-016-0043-6].

Semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] and weak supervision [@doi:10.3115/1690219.1690287] are techniques that can construct large datasets for machine learning classifiers. 
Semi-supervised learning consists of combining labeled data with unlabeled data to extract relationships.
For example, one study used a variational auto encoder with a LSTM network to extract protein-protein interactions from pubmed abstracts and full text [@arxiv:1901.06103v1].
This is an elegant solution to handle the small dataset problem, but requires labeled data to start. 
The dependency on labeled data makes finding under-studied relationships difficult as one would need to find or construct examples of the missing relationships in the beginning.

Weak or distant supervision takes a different approach that uses noisy or even erroneous labels to train classifiers [@doi:10.3115/1690219.1690287; @doi:10.1093/bioinformatics/btv476; @doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898].
Under this paradigm sentences are labeled based on their mention pair being present (positive) or absent (negative) in a database.
Once these labels are obtained a machine learning classifier can now be trained to predict sentences [@doi:10.3115/1690219.1690287].
For example, Thomas et al. [@raw:Thomas2011LearningTE] used distant supervision to train a support vector machine to extract sentences mentioning protein-protein interactions (ppi). 
Their SVM model achieved comparable performance against a baseline model; however, the noise generated via distant supervision was difficult to eradicate [@raw:Thomas2011LearningTE].
A number of efforts have focused on combining distant supervision with other types of labeling strategies to reduce the negative impacts of noisy knowledge bases [@arxiv:1805.09927; @arxiv:1805.09929; @doi:10.18653/v1/W17-2323].
Nicholson et al. [@doi:10.1101/730085] found that, in some circumstances, these strategies and rules can be reused across different types of biomedical edges to learn a heterogeneous knowledge graph if those edges describe similar physical concepts.
This remains an active area of investigation with numerous associated challenges and opportunities.
Overall, semi-supervised learning and weak supervision provide promising results in terms of relation extraction and future approaches should consider using those paradigms to train machine learning classifiers.

| Dataset | Type of Sentences |
| --- | --- |
| AIMed [@doi:10.1016/j.artmed.2004.07.016] | PPI |
| BioInfer [@doi:10.1186/1471-2105-8-50] | PPI | 
| LLL [@raw:LLL] | PPI |
| IEPA [@raw:IEPA] | PPI |
| HPRD5 [@doi:10.1093/bioinformatics/btl616] | PPI |
| EU-ADR [@doi:10.1016/j.jbi.2012.04.004] | DaG |
| BeFree [@doi:10.1186/s12859-015-0472-9] | DaG |
| CoMAGC [@doi:10.1186/1471-2105-14-323] | DaG | 
| CRAFT [@doi:10.1186/1471-2105-13-161] | DaG |
| Biocreative V CDR [@doi:10.1093/database/baw068] | Compound induces Disease |
| Biocreative IV ChemProt [@raw:biocreative/chemprot] | CbG |

Table: A set of publicly available datasets for supervised text mining. {#tbl:supervised-text-datasets}
