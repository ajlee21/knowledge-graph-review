## Building Biomedical Knowledge Graphs

1. Set up the context for relationship extraction
	1. Define the problem
	2. Talk about the importance of the problem (filling knowledge bases -> point researchers to relevant papers)
2. Give overview towards taxonomy of approaches (hand written rules, unsupervised machine learning, supervised machine learning etc.)

### Constructing Databases and Manual Curation

1. Talk about papers that construct knowledge graphs without text mining approaches
2. Discuss the positives and negatives for these methods

### Text Mining for Relationship Extraction

#### Rule-Based Natural Language Processing

1. Mention papers on hand written rules and expressions

#### Unsupervised Machine Learning

Unsupervised machine learning consists of finding hidden patterns within data without labels.
Common methods of unsupervised learning involve clustering, statistical calculations or neural network methods such as autoencoders.
In this section we discuss unsupervised methods used to identify relationship asserting sentences. 

Most of these unsupervised methods use a form of co-occurrence to detect sentences of interest (Table {@tbl:unsupervised-methods-text-mining}).
Co-occurrence exploits the customary trend that two entities can appear together in text.
Two popular databases DISEASES [@doi:10.1016/j.ymeth.2014.11.020] and STRING [@doi:10.1093/nar/gku1003] were populated using a co-occurrence approach to extract disease-gene associations and protein-protein interactions from PubMed abstracts.
This approach successfully captured a significant amount of associations (high recall), but this metric wouldn't be able to capture rare associations.
 
Besides PubMed abstracts there was a study that used a similar approach to text mine full articles [@doi:10.1371/journal.pcbi.1005962].
The authors used 15 million full text articles to detect protein-protein interactions and other protein related information.
They discovered that using full text articles improves detection performance.
This improvement opens a direction people should go for future experiments, but caution should be taken as converting full text is a non-trivial task. <font color=red> <-FIX THIS SENTENCE </font>
This approach was able to recall a significant amount of interactions and associations, but suffers in terms of precision.

Besides co-occurence, one study took a clustering approach to detect sentences. 
This study generated dependency trees for each sentence contain in PubMed abstracts.
Upon generation,reserchers used biclustering to group relevant sentences together [@doi:10.1093/bioinformatics/bty114].
This study was able to recall a significant amount of groups.
A caveat towards this approach is that a lot of dependency paths do not overlap with eachother.
This results in failure to capture rare or under reported relationships.

| Study | Short Description |
| --- | --- |
| [@doi:10.1093/bioinformatics/btz490] | |
| [@doi:10.1109/bibm.2015.7359766] | | 
| [@doi:10.1371/journal.pcbi.1000943] | | 
| [@doi:10.1371/journal.pcbi.1005962]| |
| [@doi:10.1016/j.ymeth.2014.11.020] | |
| [@doi:10.1093/nar/gku1003]| | 
| [@doi:10.1371/journal.pcbi.1005017] | | 

Table: Table of approaches that mainly use a form of co-occurrence. {#tbl:unsupervised-methods-text-mining}

#### Supervised Machine Learning

1. Mention the availablility of publically available data
	1. PPI - 5 datasets 
	   1. 10.1016/j.artmed.2004.07.016 
	   2. 10.1186/1471-2105-8-50 
	   3. Learning language in logic - genic interaction extraction challenge
	   4. 10.1093/bioinformatics/btl616 
	   5. http://helix-web.stanford.edu/psb02/ding.pdf
	2. DaG - 3 datasets
	   1. 10.1016/j.jbi.2012.04.004 
	   2. 10.1186/s12859-015-0472-9
	   3. 10.1186/1471-2105-14-323 
	   4. 10.1186/1471-2105-13-161
	3. CiD 
	  1. 10.1093/database/baw068 
	4. CbG 
	  1. Biocreative VI track 5 - raw citation
	5. more if exists talk about deep learning methods
2. Mention the use of Support Vector Machines and other non deep learning classifiers
   1. Will have to mention that field has moved to deep learning.
   2. 10.1186/s13326-017-0168-3
   3. 10.1371/journal.pcbi.1004630
3. Mention deep learning methods
   1. 1901.06103v1
   2. 10.1016/j.knosys.2018.11.020
   3. 10.1177/0165551516673485
   4. 1706.01556v2
   5. ^^ A few papers here but a lot more will be put into place 
   6. Mention caveat which is the need for large annotated datasets
   7. Mention a direction the field is moving to which is weak supervision and more that info that will come in time.
