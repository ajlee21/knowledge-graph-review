## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

1. Mention techniques for these with some papers

#### Deep Learning

Deep learning techniques have flourished thanks to  popular neural network model used in natural language processing called Word2Vec [@arxiv:1310.4546; @arxiv:1301.3781].
Word2Vec project words into a low dimensional space and
this space have perserves their semantic meaning.
Two neural network models that Word2Vec can use are skip-gram and continuous bag of words (CBOW).
These models are feed-forward neural networks that are trained to predict a word given it's context window  (CBOW) or predict the context window given a word (skip-gram).
This prediction process tunes the neural network to learn the context of words and consequently capture semantic meaning as well.
Once training has finished, the weights of these networks are then used to represent each word.
Majority of models discussed in this section use this central idea to project nodes into a low dimensional space.

Early work consisted of using a combination of random walks with a skip-gram model to generate emebddings. 
Deepwalk [@arxiv:1403.6652] is a notable method that used this approach.
This method consisted of starting at a node and randomly traverse to other nodes for a set period of steps. 
Each visited node is used to generate the starting node's context window.
After every node has went this process a skip-gram model is trained to generate these dense vectors [@arxiv:1403.6652].
This method generated useful embeddings, but treats every node with equal transitionprobability.
This limitation results in loss of information and unclear interpretation of embeddings.
Based on the mention limitations, similar methods were constructed to solve this issue [@doi:10.1145/2736277.2741093; @arxiv:1607.00653]; however, one missing feature is that these methods do not take the edge type into account.
More recent methods have solved this issue [@doi:10.1145/3097983.3098036; @arxiv:1809.02269; @arxiv:1808.05611].

Following the skip-gram based approaches, there are other approaches that used an adjacency matrix combined with other features to project nodes into a low dimenisional space.
Some of these approaches used autoencoders[@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407] to do this projection.
Other approaches used a graph convolutional nerual network (GCN) to doing this projection [@arxiv:1609.02907].
A GCN is a neural network that uses an adjacency matrix combined with node features to produce node representaitons that capture the structure of the graph.
A catch with these methods is that they require large networks to work well.
<font color=red> figure out other caveats if one can </font>

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
