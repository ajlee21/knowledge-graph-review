## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

1. Mention techniques for these with some papers

#### Deep Learning

Deep learning is a paradigm that excels in mapping high dimensional data into a low dimensional space.
Many techniques that use deep learning for knowledge graphs are based on Word2Vec [@arxiv:1310.4546; @arxiv:1301.3781], a set of approaches that are popular for natural language processing.
The goal of a Word2Vec model is to project words into a low dimensional space that preserves their semantic meaning.
Strategies for Word2Vec use one of two neural network models, skip-gram and continuous bag of words (CBOW).
Both models are feed-forward neural networks, but CBOW models are trained to either predict a word given it's context while skip-gram models are trained to predict the context given a word.
Once either network has been trained, the finalized network weights are used to represent words and their meaning.

Deepwalk [@arxiv:1403.6652] is an early method designed to project a knowledge graph into a low dimensional space. 
First step of this method is to undergo a random walk along a knowledge graph.
During the random walk, every generated sequence of nodes is recorded and treated like a sentence in the word2vec model [@arxiv:1310.4546; @arxiv:1301.3781].
After every node has been processed, a skip-gram model is trained to predict the context of each node thereby projecting a knowledge graph into a low dimensional space [@arxiv:1403.6652].
A caveat to this method is that the random walk takes an unbiased way to traverse each node.
This may not be suitable given for some knowledge graph applications.
Newer approaches use a modified random walk algorithm [@doi:10.1145/2736277.2741093; @arxiv:1607.00653]; however, these methods do not currently account for different edge and node types.
Certainly newly designed approaches can also use information about node and edge types [@doi:10.1145/3097983.3098036; @arxiv:1809.02269; @arxiv:1808.05611].
These approaches primarily capture a network's local structure: more work is needed to build embeddings that capture both the local and global structure of a network.

Some deep learning approaches use an adjacency matrix as input [@arxiv:1310.4546; @arxiv:1301.3781] instead of using the word2vec framing.
Algorithms such as auto-encoders can also generate network embeddings [@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407].
Autoencoders [@arxiv:1404.7828] are neural networks that map adjacency matrices into a low dimensional space.
This neural network is trained by mapping the matrix into a low dimensional span and then reconstructs the same matrix.
The generated latent space captures the structure of the knowledge graph and the nodes are mapped onto this space.
Despite the high potential of this approach, this method relies on an adjacency matrix for input.
If a knowledge graph asymtotically increases in size, these approaches could run into scalability issues.
Future approaches should consider generating a more scalable approach for these knowledge graph embeddings as well as incorporate other information such as edge and node types.

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
