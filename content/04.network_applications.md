## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

1. Mention techniques for these with some papers

#### Deep Learning

Deep learning is a paradigm that excels in mapping high dimensional data into a low dimensional space.
Most techniques under this paradigm have flourished based on a popular natural language processing approach called Word2Vec [@arxiv:1310.4546; @arxiv:1301.3781].
Word2Vec projects words into a low dimensional space that preserves their semantic meaning.
This approach uses one of two neural network models, skip-gram and continuous bag of words (CBOW), to accomplish this projection task.
Both models are feed-forward neural networks that are trained to either predict a word given it's context (CBOW) or the context given a word (skip-gram).
Once either network has been trained, the finalized network weights are used to represent words and their meaning.
We discuss approaches that use this central idea to project nodes into a low dimensional space.

Deepwalk [@arxiv:1403.6652] was one of the originating methods to project a knowledge graph into a low dimensional space. 
First step of this method is to undergo a random walk along a knowledge graph to ascertain how nodes are connected to one another.
During the random walk, every generated sequence of nodes is recorded and treated like a sentence in the word2vec model [@arxiv:1310.4546; @arxiv:1301.3781].
After every node has been processed, a skip-gram model is trained to predict the context of each node thereby projecting a knowledge graph into a low dimensional space [@arxiv:1403.6652].
A caveat to this method is that the random walk takes an unbiased way to traverse each node.
This may not be suitable given for some knowledge graph applications.
Some approaches fixed this problem by updating their random walk algorithm [@doi:10.1145/2736277.2741093; @arxiv:1607.00653]; however, these methods do not take edge type and node type into account.
Based on this problem, other approaches have evolved to take edge and type information into account [@doi:10.1145/3097983.3098036; @arxiv:1809.02269; @arxiv:1808.05611].
Most of these approaches capture a networks local structure, but a future direction is to consider creating embeddings that capture both the local and global structure of a network.

Besides using Word2Vec [@arxiv:1310.4546; @arxiv:1301.3781] inspired approaches, some deep learning approaches can use an adjacency matrix as input.
Algorithms such as auto-encoders can be used to generate embeddings [@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407].
Autoencoders [@arxiv:1404.7828] are neural networks that map adjacency matrices into a low dimensional space.
This nerual network is trained by mapping the matrix into a low dimensional span and then reconstructs the same matrix.
The generated latent space captures the structure of the knowledge graph and the nodes are mapped onto this space.
Despite the high potential of this approach, this method relies on an adjacency matrix for input.
If a knowledge graph asymtotically increases in size, these approaches could run into scalability issues.
Future approaches should consider generating a more scalable approach for these knowledge graph embeddings as well as incorporate other information such as edge and node types.

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
