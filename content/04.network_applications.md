## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

1. Mention techniques for these with some papers

#### Translational Distance Models

Translational distance models view edges in a knowledge graph as linear transformations.
Every node-edge pair is considered a triplet with head nodes represented as $\textbf{h}$, edges represented as $\textbf{r}$, and tail nodes represented as $\textbf{t}$.
These representations are combined into an equation that mimics the iconic word vectors translations ($\textbf{king} - \textbf{man} + \textbf{woman} \approx \textbf{queen}$) from the Word2vec model [@arxiv:1310.4546].
This equation is shown as follows: $\textbf{h} + \textbf{r} \approx \textbf{t}$ [@raw:bordestranse].
Starting at the head node ($\textbf{h}$), add the edge vector ($\textbf{r}$) and the result should be the tail node ($\textbf{t}$).
Embeddings for $\textbf{h}$, $\textbf{r}$, $\textbf{t}$ are optimized while guaranteeing the above equation is satisfied.
In this section we discuss work that first utilized this viewpoint and mention selected variants of this approach.

TransE [@raw:bordestranse] is a method that uses the above perspective to project knowledge graphs into a low dimensional space.
TransE uses every edge in a network and optimizes h, r, and t embeddings, while preserving the global equation ($\textbf{h} + \textbf{r} \approx \textbf{t}$).
A caveat to this approach is that it the training steps force relationships to have a one to one mapping, which may not be appropriate for all types of relationships.
Wang et al. [@raw:wangtransH] worked on resolving this issue by developing the TransH model.
TransH treats relations as hyperplanes rather than a regular vector and projects the head ($\textbf{h}$) and tail ($\textbf{t}$) nodes onto the hyperplane.
Following this projection, a distance vector ($\textbf{d}_{r}$) is calculated between the projected head and tail nodes.
Finally, each vector is optimized while preserving the global equation ($\textbf{h} + \textbf{d}_{r} \approx \textbf{t}$) [@raw:wangtransH].
Other approaches [@raw:lintransR, @arxiv:1610.04073; @arxiv:1909.00672] have built off of the TransE and TransH models. 
Overall these methods take an interesting perspective concerning projecting knowledge graphs into a low dimensional space.
Possible future direction for these models is to incorporate other types of information such as confidence scores of an edge, textual information, or even the edge type itself when optimizing these knowledge graph embeddings.

#### Deep Learning

1. Define node neighborhoods
2. Talk about random walks 
3. Talk about auto encoders random walk independent approaches 

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
