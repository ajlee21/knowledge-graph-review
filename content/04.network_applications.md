## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

Mapping high dimensional data into a low dimensional space has greatly improved modeling performance in fields such as natural language processing [@arxiv:1301.3781; @arxiv:1310.4546] and image analysis [@arxiv:1512.03385].
The success of these approaches provides rationale for projecting knowledge graphs into a low dimensional space as well [@arxiv:1709.05584].
Techniques that perform this projection often require information on how nodes are connected with one another [@raw:gongreplearn; @doi:10.1109/TKDE.2016.2606098; @raw:hanreplearn; @doi:10.1145/2806416.2806512], while other approaches can work directly with the edges themselves [@arxiv:1610.04073].
We group methods for producing low-dimensional representations of knowledge graphs into the following three categories: matrix factorization, translational methods, and deep learning (Figure {@fig:unifying_techniques_overview}).

![
Pipeline for embedding knowledge graphs into a low dimensional space.
Starting with a knowledge graph, embeddings can be generated using one of the following options: Matrix Factorization (a), Translational Models (b) or Deep Learning (c).
The output of this pipeline is an embedding space that clusters similar node types together.
](images/figures/unifying_techniques_overview.png){#fig:unifying_techniques_overview}

#### Matrix Factorization

Matrix factorization is a technique that uses linear algebra to map high dimensional data into a low dimensional space.
This projection is accomplished by decomposing a matrix into a set of small rectangular matrices (Figure {@fig:unifying_techniques_overview} (a)).
Notable methods for matrix decomposition include Isomap [@doi:10.1126/science.290.5500.2319], Laplacian eigenmaps [@doi:10.1162/089976603321780317] and Principal Component Analysis (PCA) [@doi:10/bm8dnf]/Singular Vector Decomposition (SVD) [@doi:10.1007/BF02288367].
In this section we discuss approaches that utilize the above methods to project knowledge graphs into a low dimensional space.

SVD [@doi:10.1007/BF02288367] is an approach that can project knowledge graphs into a low dimensional space.
The input for this technique is an adjacency matrix ($A$), which is a square matrix where rows and columns represent nodes and each entry represents the presence of an edge between two nodes. 
This adjacency matrix ($A$) gets decomposed into three parts: a square matrix $Σ$ and a set of two small rectangular matrices $U$ and $V^{T}$.
This values within $Σ$ are called singular values, which akin to eigenvalues [@doi:10.1007/BF02288367].
Each row in $U$ and each column in $V^{T}$ represents nodes projected onto a low dimensional space [@doi:10.1007/BF02288367; @doi:10/bm8dnf].
In practice $U$ is usually used to represent nodes in a knowledge graph, but $V^{T}$ can also be used [@doi:10.1007/BF02288367; @raw:Jiezhon2018].
Overall, SVD is a universal algorithm and is typically used as a baseline to compare to other approaches [@arxiv:1906.05017].

Laplacian eigenmaps is an approach that projects knowledge graphs into a low dimensional space while preserving the graph's local structure.
Similar to SVD this approach takes an adjacency matrix ($A$) as input as well as a degree matrix ($D$): a diagonal matrix where each entry represents the number of edges connected to a node.
The adjacency matrix is converted into a laplacian matrix ($L$) by subtracting it from the degree matrix $L=D-A$.
Once the laplacian matrix is constructed, this algorithm uses linear algebra to calculate eigenvalues and eigenvectors from the matrix ($Lx = \lambda Dx$).
The generated eigenvectors represent nodes projected onto a low dimensional space [@doi:10.1162/089976603321780317].
A number of approaches have used variants of this algorithm to perform their own node projection [@raw:gongreplearn; @doi:10.1109/TKDE.2016.2606098; @arxiv:1905.09763].
Typically eigenmaps work well when knowledge graphs have a sparse number of edges between nodes and struggle with denser networks [@arxiv:1906.05017; @doi:10.1371/journal.pcbi.1005621; @arxiv:1905.09763].
A future direction is to adapt these methods to scale to knowledge graphs that have a large number of edges.

Matrix factorization is a powerful technique that uses a matrices such as  an adjacency matrix as input.
Common approaches involve using SVD, Laplacian eigenmaps or variants of the two to perform embeddings.
Despite reported success, the dependence on matrices like an adjacency matrix creates an issue of scalability.
Furthermore, these methods treat all edge types the same, but a possible extension would be to incorporate node and edge types as sources of input.

#### Translational Distance Models

Translational distance models treat edges in a knowledge graph as linear transformations.
As an example, one such algorithm, TransE [@raw:bordestranse], treats every node-edge pair as a triplet with head nodes represented as $\textbf{h}$, edges represented as $\textbf{r}$, and tail nodes represented as $\textbf{t}$.
These representations are combined into an equation that mimics the iconic word vectors translations ($\textbf{king} - \textbf{man} + \textbf{woman} \approx \textbf{queen}$) from the Word2vec model [@arxiv:1310.4546].
The equation is shown as follows: $\textbf{h} + \textbf{r} \approx \textbf{t}$.
Starting at the head node ($\textbf{h}$), add the edge vector ($\textbf{r}$) and the result should be the tail node ($\textbf{t}$).
TransE optimizes embeddings for $\textbf{h}$, $\textbf{r}$, $\textbf{t}$, while guaranteeing the global equation ($\textbf{h} + \textbf{r} \approx \textbf{t}$) is satisfied [@raw:bordestranse].
A caveat to the TransE approach is that it the training steps force relationships to have a one to one mapping, which may not be appropriate for all types of relationships.

Wang et al. [@raw:wangtransH] attempted to resolve the one to one mapping issue by developing the TransH model.
TransH treats relations as hyperplanes rather than a regular vector and projects the head ($\textbf{h}$) and tail ($\textbf{t}$) nodes onto the hyperplane.
Following this projection, a distance vector ($\textbf{d}_{r}$) is calculated between the projected head and tail nodes.
Finally, each vector is optimized while preserving the global equation ($\textbf{h} + \textbf{d}_{r} \approx \textbf{t}$) [@raw:wangtransH].
Other approaches [@raw:lintransR, @arxiv:1610.04073; @arxiv:1909.00672] have built off of the TransE and TransH models. 
In the future, it may be beneficial for these models is to incorporate other types of information such as edge confidence scores, textual information, or edge type information when optimizing these embeddings.

#### Deep Learning

Deep learning is a paradigm that uses multiple non-linear transformations to map high dimensional data into a low dimensional space.
Many techniques that use deep learning for knowledge graphs are based on word2vec [@arxiv:1310.4546; @arxiv:1301.3781], a set of approaches that are widely used for natural language processing.
The goal of word2vec is to project words into a low dimensional space that preserves their semantic meaning.
Strategies for training word2vec models use one of two neural network architectures: skip-gram and continuous bag of words (CBOW).
Both models are feed-forward neural networks, but CBOW models are trained to predict a word given it's context while skip-gram models are trained to predict the context given a word [@arxiv:1310.4546; @arxiv:1301.3781].
Once training has finished, words are now associated with dense vectors that downstream models, such as feed forward networks or recurrent networks, can use for input.

Deepwalk [@arxiv:1403.6652] is an early method designed to project a knowledge graph into a low dimensional space. 
The first step of this method is to perform a random walk along a knowledge graph.
During the random walk, every generated sequence of nodes is recorded and treated like a sentence in word2vec [@arxiv:1310.4546; @arxiv:1301.3781].
After every node has been processed, a skip-gram model is trained to predict the context of each node thereby projecting a knowledge graph into a low dimensional space [@arxiv:1403.6652].
A limitation of this method is that the random walk cannot be controlled, so every node has an equal chance to be reached.
Grover and Leskovec [@arxiv:1607.00653] demonstrated that this limitation can hurt performance when classifying edges between nodes and developed node2vec as a result.
Node2vec [@arxiv:1607.00653] operates the in the same fashion as deepwalk; however, this algorithm specifies a parameter that lets the random walk be biased when traversing nodes.
A caveat to both deepwalk and node2vec is that both algorithms ignore information such as edge type and node type.
Various approaches have evolved to fix this limitation by incorporating  node, edge and even path types when projecting nodes into a low dimensional space [@arxiv:1704.03165; @doi:10.1145/3097983.3098036; @arxiv:1809.02269; @arxiv:1808.05611].
These approaches primarily capture a network's local structure. 
An emerging area of work is to develop approaches that capture both the local and global structure of a network when projecting knowledge graphs into a low dimensional space.

Some deep learning approaches use an adjacency matrix as input [@arxiv:1310.4546; @arxiv:1301.3781] instead of using the word2vec framing.
Algorithms such as auto-encoders can also generate network embeddings [@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407].
Autoencoders [@arxiv:1404.7828; @raw:Baldi2011] are neural networks that map input such as an adjacency matrices into a low dimensional space and then learns how to construct this space by reconstructing the same input.
The generated low dimensional space captures the node connectivity structure of the knowledge graph and every node is mapped onto this space [@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407].
Despite the high potential of this approach, this method relies on an adjacency matrix for input.
If a knowledge graph asymptotically increases in size, these approaches could run into scalability issues as discovered by Khosla et al. [@arxiv:1903.07902].
Plus, Khosla et al.[@arxiv:1903.07902] discovered that approaches akin to node2vec outperformed algorithms using autoencoders when undergoing link prediction and node classification.
Overall, the performance of these models largely depends upon the structure of nodes and edges within a knowledge graph [@arxiv:1903.07902].
Future approaches should consider creating hybrid models that use both node2vec and autoencoders to construct complementary low dimensional representations of knowledge graphs.

### Unifying Applications

Knowledge graphs have been used in many biomedical applications ranging from identifying protein functions [@doi:10.1186/s12859-018-2163-9] to prioritizing cancer genes [@doi:10.1093/bioinformatics/bty148] to recommending safer drugs to patients [@arxiv:1710.05980; @doi:10.1609/aaai.v33i01.33011126].
In this section we discuss how knowledge graphs are being applied in biomedical settings. 
We put particular emphasis on an emerging set of techniques: those that project knowledge graphs into a low dimensional space.

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
