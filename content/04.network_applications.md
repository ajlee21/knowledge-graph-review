## Applying Knowledge Graphs to Biomedical Challenges

1. Mention that these graphs can be used for discovery
2. Mention representation learning (aka representing a graph as dense vectors for nodes and/or edges)
3. 

### Unifying Techniques

1. Set up the problem that maps a knowledge graph into a low dimensional space

#### Matrix Factorization

1. Mention techniques for these with some papers

#### Deep Learning

Deep learning techniques have flourished thanks to a popular natural language processing approach called Word2Vec [@arxiv:1310.4546; @arxiv:1301.3781].
Word2Vec projects words into a low dimensional space that perserves their semantic meaning.
This approach uses two neural network models, skip-gram and continuous bag of words (CBOW), to accomplish this projection task.
Both models are feed-forward neural networks that are trained to either predict a word given it's context window (CBOW) or the context window given a word (skip-gram).
After training these networks, the finalized weights are used to represent words and their semantic meaning.
We discuss approaches that use this central idea to project nodes into a low dimensional space.

Projecting a network into a low dimensional space involves a combination of random walks and skip-gram model. 
Deepwalk [@arxiv:1403.6652] is one of the originating methods that use this approach.
The method's first step is to start at one node and randomly traversing to other nodes for a set period of steps.
Each visited node is recorded and used to generate the starting node's context window.
After processing every node, a skip-gram model is trained to project nodes into a low dimensional space [@arxiv:1403.6652].
Deepwalk takes an unbiased way to traverse each node, which may not be suitable for all types of approaches.
Some approaches have evolved to circumvent this unbiased bottleneck [@doi:10.1145/2736277.2741093; @arxiv:1607.00653].
Despite change the bias random walk, these methods don't take edge type into account.
As a consequence there are approaches that have shifted to fixing this issue [@doi:10.1145/3097983.3098036; @arxiv:1809.02269; @arxiv:1808.05611].
Most of these approaches capture a networks local structure, but a future direction to consider is creating embeddings that capture both the local and global structure of a network.

Network projection doesn't have to use a random walk as the first step.
Some approaches use an an adjacency matrix as input.
Once an adjancency matrix is provided, algorithms such as autoencoders [@arxiv:1802.08352; @arxiv:1611.07308; @arxiv:1802.04407] can be used to generate embeddings.
Autoencoders [@arxiv:1404.7828] are networks that map adjacency matricies into a low dimensional space and then aims to reconstruct the same matrix.
After training, the generated latent space can be used to represent each node of a network.
Besides autoencoders another approach is to use a graph convolutional network [@arxiv:1609.02907].
This network is a neural network that uses an adjacency matrix and possibly other forms of information to produce node emebddings.
Despite the high potential of these approaches, these methods require large networks to work well.
Plus, this method rely on an adjacency matrix for input, which results in scalability issues.
Future approaches should consider generating a more scalable approach for these network embeddings.

### Unifying Applications

1. Mention how the previous section is used in a biomedical setting

#### Disease and Gene Interactions

1. Mention disease gene prioritization
2. Mention Disease gene associations

#### Protein Protein Interactions

1. Mention predicting genes interacting genes

#### Drug Interactions

1. Talk about drug side effects
2. Drug repurposing
3. Drug-Disease Interations

#### Clinical applications

1. Can mention EHR use and other related applications
2. Mention Tiffany's work on private data embeddings
